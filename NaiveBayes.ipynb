{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4a5ce2c-1f9b-4b7d-beb2-3a7b942e8e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나이브 베이즈 분류기 테스트를 위한 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f583f37-ffcc-43ab-9fa1-04c23eed5ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import tqdm\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from konlpy.tag import Mecab, Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abd7d6db-e7cf-4a34-92af-6c0d81f1206d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요 데이터 로드\n",
    "df = pd.read_csv('raw.csv')\n",
    "\n",
    "with open('data.json','r') as f:\n",
    "    dataset = json.load(f)\n",
    "    \n",
    "with open('valid_data.json','r') as f:\n",
    "    valid_dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e2988a-cedb-447c-9e68-1b8d7fd1a17a",
   "metadata": {},
   "source": [
    "### TF-IDF 다시 정의해 나이브 베이즈 대입하는 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58c0a71e-0831-41df-8da2-75b6d48b31d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#형태소 분석기\n",
    "flag = 0\n",
    "\n",
    "try:\n",
    "    Ko_Analyzer = Mecab(dicpath = r\"C:/mecab/mecab-ko-dic\")\n",
    "    flag = 1\n",
    "except:\n",
    "    Ko_Analyzer = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5234fd83-7b0f-46ea-9efe-e9c00efe74c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = 0\n",
    "Ko_Analyzer = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "451c4344-c6ba-4ba5-8431-8bb684768289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 전처리 관련 함수 정의\n",
    "def text_preprocessing(text):\n",
    "    # 불용어(Stopword)는 공통된 단어 혹은 비교 기준 명사로 쓰일 수 없는 것들을 사용한다.\n",
    "    stop_words = ['은', '는', '이', '가', '하', '아', '것', '들', '의','번','장','방','시','에서','우리',\n",
    "                  '년', '있', '되', '수', '보', '주', '등', '한', '및','면','뿐','나','그것','이것','때문',\n",
    "                  '마찬가지','곳','로','와','만','데','한때','이후','이전','지금','때','각종','한편','대부분',\n",
    "                  '를','앞','후','경우','간','반면','채','어젠','서','해당','과','외','예','중','사','조','항','제',\n",
    "                  '뒤','현재','무엇','중','그','명','예로','군데','내','너','여기','만큼','동안','듯','을','그','개',\n",
    "                  '지','여도','덕분','최근', '밖', '이상', '처음', '최소','당시','결국',\n",
    "                 # 여기서 부터는 공통 명사\n",
    "                  '사회', '필요', '지원', '관련', '정책', '정부', '결과', '연구', '분석', '국가', '문제',\n",
    "                  '이번', '북한', '기술', '기업', '정보'\n",
    "                 ]\n",
    "    \n",
    "    # 한글 및 공백을 제외한 문자 제거(영문자, 특수문자)\n",
    "    text = re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣ]\", \" \", text)\n",
    "    temp = []\n",
    "    \n",
    "    # Mecab 형태소 분석기를 사용한 품사 태깅\n",
    "    if flag == 1:\n",
    "        pos_data = Ko_Analyzer.pos(text)\n",
    "        # 품사 태깅한 것들 중 일반 명사, 지정 명사 만을 사용\n",
    "        for i in pos_data:\n",
    "            if i[1] == 'NNG' or i[1] == 'NNP':\n",
    "                temp.append(i[0])\n",
    "            else : continue\n",
    "    else:\n",
    "        pos_data = Ko_Analyzer.pos(text, norm = True)\n",
    "        for i in pos_data:\n",
    "            if i[1] == 'Noun':\n",
    "                temp.append(i[0])\n",
    "            else : continue\n",
    "    \n",
    "    \n",
    "    #nouns = mecab.nouns(text)  # 명사 추출\n",
    "    \n",
    "    # 명사 중 불용어에 해당되면 삭제\n",
    "    nouns = [token for token in temp if not token in stop_words]\n",
    "    \n",
    "    return list(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e65bdb3e-1418-4c41-9392-9d96f3d87447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label 리스트 화 및 중복 제거 과정\n",
    "label_list = list(set(dataset['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d83c6734-369f-4388-8262-e38cb9ebe132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a74add9f-ac62-4210-a002-edb0f9c7f652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨별 content 데이터 셋 초기화\n",
    "label_dataset = {'label' : [], 'content' : []}\n",
    "\n",
    "# doc_name_list에 따른 문서 제목, 내용 입력\n",
    "for i in label_list:\n",
    "    label_dataset['label'].append(i)\n",
    "    temp = list(df.loc[df['kdc_label'] == i]['passage'].values[:])\n",
    "    temp = \" \".join(temp)\n",
    "    label_dataset['content'].append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fd1fed-2128-4f39-8f61-37e4ac44f2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DTM with default tokenizer\n",
    "dtmvector = CountVectorizer(ngram_range = (1,1))\n",
    "X_train_dtm = dtmvector.fit_transform(label_dataset['content'])\n",
    "\n",
    "# TF-IDF 생성\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidffv = tfidf_transformer.fit_transform(X_train_dtm)\n",
    "print(tfidffv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "39c82d9e-5f31-432f-8acc-a11a2a093503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99, 12447922)\n"
     ]
    }
   ],
   "source": [
    "# DTM with custom tokenizer\n",
    "dtmvector = CountVectorizer(tokenizer = text_preprocessing, ngram_range = (2,3))\n",
    "X_train_dtm = dtmvector.fit_transform(label_dataset['content'])\n",
    "\n",
    "print(X_train_dtm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dcb44ec3-4c55-4ac1-9913-122d71568fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99, 12447922)\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF 생성\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidffv = tfidf_transformer.fit_transform(X_train_dtm)\n",
    "print(tfidffv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad92127-4d43-46d8-8066-43b5a8914da2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 다항 나이브 베이즈 분류기(alpha = 0.5)\n",
    "mod_2 = MultinomialNB(alpha = 0.5, class_prior = None, fit_prior = True)\n",
    "mod_2.fit(tfidffv, label_dataset['label']) # fit : X, Y에 맞춰서 분류기를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2cdd84c6-7e34-4b37-8259-fa59a6ad31f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 다항 나이브 베이즈 분류기(alpha = 1.0)\n",
    "mod = MultinomialNB(alpha = 1.0, class_prior = None, fit_prior = True)\n",
    "mod.fit(tfidffv, label_dataset['label']) # fit : X, Y에 맞춰서 분류기를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c6e24b3c-8533-49a0-92f8-5330dfad982b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## test dataset 만들기..중복없이 500개의 랜덤 샘플 생성\n",
    "random_idx = []\n",
    "random_num = random.randint(0,3998)\n",
    "\n",
    "for i in range(500):\n",
    "    while random_num in random_idx:\n",
    "        random_num = random.randint(0,3998)\n",
    "    random_idx.append(random_num)\n",
    "\n",
    "test_dataset = {'label' : [], 'content' : []}\n",
    "\n",
    "# 랜덤 인덱스에 따른 새로운 test dataset 생성\n",
    "for i in random_idx:\n",
    "    test_dataset['label'].append(valid_dataset['label'][i])\n",
    "    test_dataset['content'].append(valid_dataset['content'][i])\n",
    "    \n",
    "#print(test_dataset['label'][0])\n",
    "#print(test_dataset['content'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "048bb61d-0058-482e-936a-0da885807f0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 12447922)\n",
      "(500, 12447922)\n",
      "정확도: 0.922\n",
      "Wall time: 1min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# time을 사용해 해당 셀(분류기 예측)에 대한 성능 및 시간 테스트\n",
    "# Test dataset에 대한 tf-idf 생성 과정\n",
    "X_test_dtm = dtmvector.transform(test_dataset['content'])\n",
    "\n",
    "print(X_test_dtm.shape)\n",
    "\n",
    "tfidfv_test = tfidf_transformer.transform(X_test_dtm)\n",
    "print(tfidfv_test.shape)\n",
    "\n",
    "# 예측\n",
    "predicted = mod.predict(tfidfv_test)\n",
    "print(\"정확도:\", accuracy_score(test_dataset['label'],predicted))\n",
    "\n",
    "#print(predicted)\n",
    "#print(test_dataset['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359eceb7-a398-4bd8-bacd-d98e95603cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# time을 사용해 해당 셀(분류기 예측)에 대한 성능 및 시간 테스트\n",
    "# Test dataset에 대한 tf-idf 생성 과정\n",
    "X_test_dtm = dtmvector.transform(test_dataset['content'])\n",
    "\n",
    "print(X_test_dtm.shape)\n",
    "\n",
    "tfidfv_test = tfidf_transformer.transform(X_test_dtm)\n",
    "print(tfidfv_test.shape)\n",
    "\n",
    "# 예측\n",
    "predicted = mod_2.predict(tfidfv_test)\n",
    "print(\"정확도:\", accuracy_score(test_dataset['label'],predicted))\n",
    "\n",
    "#print(predicted)\n",
    "#print(test_dataset['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d5eba9-7d7b-4e4e-906b-21cb1bab852e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "############ BY VALID ############\n",
    "<with custom tokenizer / alpha = 1.0>\n",
    "ngram = 1,1 : 0.542 / 0.564 / 0.538 / 0.568 / 0.548\n",
    "ngram = 1,2 : 0.626 / 0.632 / 0.596 / 0.66 / 0.626\n",
    "ngram = 1,3 : 0.72 / 0.694 / 0.694 / 0.694 / \n",
    "ngram = 2,3 : 0.906 / 0.902 / 0.914 / 0.914 / 0.932 (약 1분 소요)\n",
    "\n",
    "<with custom tokenizer / alpha = 0.5>\n",
    "ngram = 1,1 : 0.542 / 0.564 / 0.538 / 0.568 / 0.548\n",
    "ngram = 1,2 : 0.626 / 0.632 / 0.596 / 0.66 / 0.626\n",
    "ngram = 1,3 : 0.72 / 0.694 / 0.694 / 0.694 / \n",
    "ngram = 2,3 : 0.910 / 0.928 / 0.916 / 0.92 / 0.922 (약 1분 소요) (vector : 12447922)\n",
    "\n",
    "<with default tokenizer> : 내장된 토크나이징 기법으로는 너무 많은 벡터 생성 -> 처리 시간 느림\n",
    "ngram = 1,1 : 0.73\n",
    "ngram = 1,2 : 0.864\n",
    "ngram = 1,3 : 측정불가\n",
    "ngram = 2,3 : 측정불가\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
